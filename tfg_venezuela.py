# -*- coding: utf-8 -*-
"""TFG_Venezuela.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-cbvX2RHLG3WPVdoy4gjjSncZecT1FHG
"""

# Se installan los distintos paquetes que se van a utilizar
!pip install pandas-profiling==2.*
!pip install plotly==4.*
!pip install gensim
!pip install spacy --upgrade
!pip install pyldavis --upgrade
!pip install chart-studio --upgrade
!pip install wordcloud --upgrade
!pip install --upgrade --force-reinstall numpy
!pip install --upgrade --force-reinstall pandas
!pip install numpy==1.23.5
!pip install scipy==1.10.1
!pip install gensim==4.3.1
!pip install Unidecode
!pip install datatable --upgrade
!pip install emoji --upgrade
!pip install unidecode --upgrade
!pip install chart_studio --upgrade
!pip install pyldavis --upgrade

# Descarga de modelos de lenguaje para Spacy y NLTK
!python -m spacy download en_core_web_sm
!python -m spacy download es_core_news_sm

# Se actualizan las librerías necesarias con pip en caso de necesitarlo
!pip install --upgrade pip
!pip install --upgrade numpy
!pip install --upgrade pandas
!pip install --upgrade matplotlib
!pip install --upgrade seaborn
!pip install --upgrade emoji
!pip install --upgrade wordcloud
!pip install --upgrade pyldavis
!pip install --upgrade chart-studio
!pip install --upgrade regex
!pip install --upgrade spacy
!pip install --upgrade nltk
!pip install --upgrade autopep8
!pip install --upgrade datatable
!pip install --upgrade vaderSentiment
!pip install --upgrade unidecode

!pip install datatable --upgrade
!pip install emoji --upgrade

!pip install unidecode --upgrade
!pip install chart_studio --upgrade
!pip install pyldavis --upgrade

!pip install chart_studio --upgrade

!pip install pyldavis --upgrade

# Manejo y procesamiento de datos
import pandas as pd
import numpy as np
import json
import requests
import datatable as dt

# Limpieza de texto
import re
import string
import regex
import emoji
from unidecode import unidecode
from collections import Counter
from random import seed

# Visualización de datos
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import plotly.express as px
import chart_studio
import chart_studio.plotly as py
import chart_studio.tools as tls
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# Procesamiento de lenguaje natural (NLP)
import spacy
from spacy.tokenizer import Tokenizer
nlp = spacy.load('en_core_web_sm')

# Natural Language Toolkit
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Modelado de tópicos (LDA, Gensim)
import gensim
from gensim import models
from gensim.corpora import Dictionary
from gensim.models import LdaMulticore, CoherenceModel
from gensim.models.coherencemodel import CoherenceModel
from gensim.parsing.preprocessing import STOPWORDS as SW

# Métodos de modelado adicionales
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.model_selection import GridSearchCV

# Otros
from pprint import pprint

#1. Empieza el PRE-PROCESAMIENTO DE LOS DATOS
nlp = spacy.load("en_core_web_sm")

df_analisis = pd.read_csv('/content/df_reddit_comentarios_en_limpios.csv', sep=';')

#Se comprueba el número de comentarios que hay
print("Número de comentarios:", df_analisis.shape[0])

#Se verifica que los datos se han cargado correctamente mostrando las primeras filas
print(df_analisis.head())

#Se eliminan las stopwords en inglés
stop_words = stopwords.words('english')
print(stop_words)

#Se añaden algunas stopwords que no están en la lista
stop_words.extend(['like', 'also', 'really', 'thing', 'people', 'know', 'even', 'going', 'get', 'see', 'still', 'one', 'many', 'someone', 'something', 'lot', 'right', 'back', 'way', 'make', 'much', 'said', 'come', 'today', 'amp', 'new', 'use', 'need', 'want', 'think'])

#Se realiza la lematización y se añade de forma manual aquellas que no se hayan realizado automáticamente
def lemma_words(text):
    lemmas = []
    doc = nlp(text)
    for token in doc:
        if ((not token.is_stop) and (not token.is_punct)) and (token.pos_ != 'PRON'):
            lemmas.append(token.lemma_)

    # Se eliminan las palabras demasiado cortas
    lemmas = [i for i in lemmas if len(i) > 1]

    # Se aplana por si hay espacios extraños
    lemmas = [word for line in lemmas for word in line.split()]

    # Se filtra stopwords
    lemmas = [word for word in lemmas if word not in stop_words]

    # Se convierte a una sola cadena
    lemmas = ' '.join(lemmas)

    # Se normaliza caracteres (tildes, eñes, etc.)
    lemmas = unidecode(lemmas)

    # Se convierte todo a minúsculas
    lemmas = lemmas.lower()

    # Sustituciones específicas relevantes para la migración venezolana y ortografía británica
    replacements = {
        r"\borganization\b": "organisation",
        r"\blabor\b": "labour",
        r"\bdefense\b": "defence",
        r"\brealize\b": "realise",
        r"\bbehavior\b": "behaviour",
        r"\bmigrants\b": "migrant",
        r"\bgreenwashing\b": "greenwash",
        r"\bfav\b": "favorite",
        r"\bachievement\b": "achieve",
        r"\bacceleration\b": "accelerate",
        r"\bdiscussion\b": "discuss",
        r"\bdevelopment\b": "develop"
    }
    for pattern, replacement in replacements.items():
        lemmas = re.sub(pattern, replacement, lemmas)

    # Se revisa de nuevo stopwords (opcional)
    lemmas = lemmas.split()
    lemmas = [word for word in lemmas if word not in stop_words]

    # Se devuelve como texto final
    lemmas = ' '.join(lemmas)
    return lemmas

#Se aplica la función de la lematización
df_analisis['lemmas'] = df_analisis['cleaned_text'].apply(lemma_words)

#Se revisan las filas vacías para ver si tengo que elimanr
print("Filas antes de dropna:", df_analisis.shape[0])
print("Filas vacías en lemmas:", (df_analisis['lemmas'] == '').sum())

#Se imprime el dataset
df_analisis

# Se visualiza el número total de comentarios que hay ahora tras la limpieza de los datos
numero_de_comentarios = df_analisis.shape[0]
print("Número total de comentarios:", numero_de_comentarios)

#Se genera el csv de la columna lemas y el completo
df_analisis['lemmas'].to_csv('Lemmas_Venezuala.csv')
df_analisis.to_csv('Venezuela_limpia.csv')

#Se crea el Wordcloud con las palabras más destacadas del conjunto de datos
def plot_cloud(wordcloud):
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud)
    plt.axis("off");

def plot_cloud(wordcloud):
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()

# Se pinta el Wordcloud
wordcloud = WordCloud(width=3000, height=2000, random_state=1,
                      max_words=100, background_color='black',
                      colormap='Set3', collocations=False,
                      stopwords=STOPWORDS).generate(' '.join(df_analisis['lemmas']))
plot_cloud(wordcloud)

# 2. Análisis Descriptivo de N-Gramas

#Se lee el csv con los datos limpios obtenidos anteriormente y se filtra por la columna "lemmas" y se imprimen
df_analisis = pd.read_csv('Venezuela_limpia.csv')
dft= df_analisis['lemmas']
dft = [x for x in dft if str(x) != 'nan']
print(dft)

#Se calcula el Valor de TF-IDF de los unigramas y se imprimen
tfIdfVectorizer=TfidfVectorizer(use_idf=True, ngram_range=(1,1))
tfIdf = tfIdfVectorizer.fit_transform(dft)
names=tfIdfVectorizer.get_feature_names_out()
freqs = tfIdf.sum(axis=0).A1
result= dict(zip(names, freqs))
print(result)

#Se muestran los 30 unigramas con mayor valor TF-IDF
from operator import itemgetter
i = 0
results_sorted=sorted(result.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

#Se pintan los 30 unigramas con más TF-IDF
df_results=pd.DataFrame.from_dict(results_sorted).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results[0],df_results[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Unigramas más relevantes del corpus')

#Se realiza la misma operación con los bigramas y trigramas

#Se calcula el Valor de TF-IDF de los bigramas
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))

#Se muestran los 30 bigramas con mayor valor TF-IDF
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

#Se pintan los 30 bigramas con más TF-IDF
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del corpus')

#Trigramas
#Se calcula el Valor de TF-IDF de los trigramas
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))

#Se muestran los 30 trigramas con mayor valor TF-IDF
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

#Se pintan los 30 trigramas con más TF-IDF
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del corpus')

# 3. Modelado de Tópicos

#Se carga el csv obtenido ateriormente con los datos limpios para realizar el modelado de tópicos
df_modelado = pd.read_csv('Venezuela_limpia.csv')
print(df_modelado.head())

#Se procede a realizar la tokenización
def tokenize(text):
    text = str(text)
    tokens = text.split()
    return tokens

df_modelado['tokens'] = df_modelado['lemmas'].apply(tokenize) #Se añade una nueva columna llamada 'tokens' al DataFrame aplicando la tokenización a cada texto en la columna 'lemmas'
print(df_modelado.head()) #Se visualizan las primeras filas

#Se importa una semilla para la reproducibilidad de los resultados
seed(24)
#Además, se crea un diccionario a partir de los tokens encontrados
id2word = Dictionary(df_modelado['tokens'])

#Se filtran los extremos continuando con el proceso de limpieza
id2word.filter_extremes(no_below=2, no_above=.95)

#Se crea el objeto del corpus
corpus = [id2word.doc2bow(d) for d in df_modelado['tokens']]

# Se procede a encontrar el valor óptimo de k y para obtener los valores de coherencia
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):
    coherence_values_topic = []
    model_list_topic = []
    for num_topics in range(start, limit, step):
        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=10)
        model_list_topic.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values_topic.append(coherencemodel.get_coherence())

    return model_list_topic, coherence_values_topic

# Se aplica la función definida en el paso anterior
model_list_topic, coherence_values_topic = compute_coherence_values(
    dictionary=id2word,
    corpus=corpus,
    texts=df_modelado['tokens'],
    start=2,
    limit=15,
    step=1
)

#Se crea un csv con los valores coherencia del rango de tópicos establecido en el paso anterior
#coherence_values_topic_df = pd.DataFrame(coherence_values_topic)
#coherence_values_topic_df.to_csv('coherence_values.csv', index=False)
coherence_values_topic_df= pd.read_csv('coherence_values.csv', index_col=False)
coherence_values_topic_df

#Se crea una lista con los valores de las columnas indicados
coherence_values_topic_df=coherence_values_topic_df.iloc[:, 0].tolist()

# Genera la lista de X (por ejemplo, si coherence_values_topic_df tiene 13 elementos)
x_values = list(range(2, 2 + len(coherence_values_topic_df)))

# Selecciona el número de tópicos deseado
selected_topic = 4
selected_index = x_values.index(selected_topic)
x_selected = x_values[selected_index]
y_selected = coherence_values_topic_df[selected_index]

# Dibuja la gráfica
plt.figure(figsize=(10, 7))
estrella = mpath.Path.unit_regular_star(8)
circulo = mpath.Path.unit_circle()
verts = np.concatenate([circulo.vertices, estrella.vertices[::-1, ...]])
codes = np.concatenate([circulo.codes, estrella.codes])
cut_star = mpath.Path(verts, codes)

plt.plot(x_values, coherence_values_topic_df, '--r', marker="o", markersize=10, fillstyle='none')
plt.axvline(x=x_selected, color='b', linestyle='--')
plt.plot(x_selected, y_selected, '--r', marker=cut_star, markersize=18)
plt.xlabel('Número de tópicos')
plt.ylabel('Índice de coherencia')
#plt.show()

#Se genera el modelo LDA con el número de k elegido que en este caso k=4
k=4
model_k4 = gensim.models.LdaMulticore(corpus=corpus,
                                        id2word=id2word,
                                        num_topics=k,
                                        passes=10,
                                        random_state=23)

#Se procede a guardar y cargar todos los resultados obtenidos del modelo
#model_k4.save("model_4_topics.model")

# Se carga del modelo LDA previamente guardado (este paso se realiza en caso de ya haber runeado el código y tener guardado mi modelo en el ordenador, evitando así emplear otro modelo diferente)
model_k4_load = LdaMulticore.load("model_4_topics.model")

#Se calcula el coherence value
coherence_model_k4 = CoherenceModel(model=model_k4_load, texts=df_modelado['tokens'],
                                   dictionary=id2word, coherence='c_v')
coherence_model_k4 = coherence_model_k4.get_coherence()
print('\nCoherence Score: ', coherence_model_k4)

#Se observan las 10 palaras más utilizadas en cada uno de los 5 tópicos
print(model_k4_load.print_topics())
doc_lda = model_k4_load[corpus]
#Se filtra por palabras
words = [re.findall(r'"([^"]*)"',t[1]) for t in model_k4_load.print_topics()]
#Se crean los tópicos
topics = [' '.join(t[0:10]) for t in words]
for id, t in enumerate(topics):
    print(f"------ Topic {id} ------")
    print(t, end="\n\n")

#Se pinta la distancia intertópica con pyLDAvis
pyLDAvis.enable_notebook()
gensimvis.prepare(model_k4_load, corpus, id2word)

#Se añade una columna de LDA features al modelo cargado para que nos de la importancia de cada tópico
def document_to_lda_features(model_k4_load,document):
  topic_importance=np.array(model_k4_load.get_document_topics(document, minimum_probability=0))
  return topic_importance[:,1]

df_modelado['lda_features']=list(map(lambda doc: document_to_lda_features(model_k4_load,doc), corpus))

#Se realiza la tokenización
def topic_important(item_score):
    score=np.argmax(item_score, axis=0)
    return score

#Se aplica lo anterior y se visualiza la tabla obtenida
df_modelado['topic_dominant'] = df_modelado['lda_features'].apply(topic_important)
df_modelado

#Se genera un csv con los resultados para guardarlos ya que serán empleados en el análisis de sentimientos posteriormente
df_modelado.to_csv('topic_model_results.csv', index=False)

#Se saca el número de publicaciones por tópico
df_modelado["topic_dominant"].value_counts()

#Se dibuja la distribución de los tópicos en un histograma
plt.figure(figsize=(10,7))
ax=df_modelado["topic_dominant"].value_counts().sort_index().plot(kind='bar')
plt.ylabel('Frecuencia Absoluta')
plt.title('Distribución de tópicos en el corpus textual')
plt.show()

# Se obtienen las cuentas de frecuencia y ordénalas por índice
# Obtuve esta visualización también de Flourish ()
topic_counts = df_modelado["topic_dominant"].value_counts().sort_index()

# Nombres de los tópicos (0, 1, 2, ..., N)
topics = topic_counts.index
counts = topic_counts.values

# Colores para replicar el estilo: primero verde, intermedios gris, último rosa
colors = ['lightgreen'] + ['lightpink'] + ['grey'] + ['grey']

# Se crea la figura y el eje
fig, ax = plt.subplots(figsize=(10, 6))

# Se dibujan las barras
bars = ax.bar(topics, counts, color=colors, edgecolor='white')

# Se añaden las etiquetas de frecuencia encima de cada barra
for bar, count in zip(bars, counts):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, height + 50,  # Ajusta "+50" según el espacio
            f'{count}', ha='center', va='bottom', fontsize=11, fontweight='bold')

# Se etiquetan de los ejes y título
ax.set_xlabel('Tópicos', fontsize=12, fontweight='bold')
ax.set_ylabel('Número de publicaciones', fontsize=12, fontweight='bold')
ax.set_title('Distribución de los tópicos en el corpus textual', fontsize=14, fontweight='bold')

# Se personaliza el estilo: sin bordes superiores/derechos, ticks más grandes
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.tick_params(axis='both', which='major', labelsize=11)

plt.tight_layout()
plt.show()

#Se visualizan los bigramas de cada uno de los tópicos con mayor TF-IDF

#Tópico 1
topic_1 = df_modelado[df_modelado['topic_dominant']==0]
dft=topic_1['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes en el tópico 1')

#Tópico 2
topic_2 = df_modelado[df_modelado['topic_dominant']==1]
dft=topic_2['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 2')

#Tópico 3
topic_3 = df_modelado[df_modelado['topic_dominant']==2]
dft=topic_3['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 3')

#Tópico 4
topic_4 = df_modelado[df_modelado['topic_dominant']==3]
dft=topic_4['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 4')

#Se realiza lo mismo que antes pero para los trigramas de cada uno de los tópicos

#Tópico 1
topic_1 = df_modelado[df_modelado['topic_dominant']==0]
dft=topic_1['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 1')

#Tópico 2
topic_2 = df_modelado[df_modelado['topic_dominant']==1]
dft=topic_2['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 2')

#Tópico 3
topic_3 = df_modelado[df_modelado['topic_dominant']==2]
dft=topic_3['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 3')

#Tópico 4
topic_4 = df_modelado[df_modelado['topic_dominant']==3]
dft=topic_4['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se realiza el histograma
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 4')